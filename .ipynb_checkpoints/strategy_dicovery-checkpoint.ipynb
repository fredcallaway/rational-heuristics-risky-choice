{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sn;\n",
    "pd.set_option(\"display.max_columns\", 101)\n",
    "import pdb\n",
    "import time\n",
    "import pickle\n",
    "import importlib\n",
    "import process_data as p_d\n",
    "import make_figures as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data by calculatiing a variety of features, and run k-means clustering\n",
    "# processing the model data may take many minutes (this can be run outside of a notebook for ease)\n",
    "\n",
    "p_d.process_data(isHuman=True, in_file='data/human/1.0/trials.csv', out_dir='data/human/1.0/')\n",
    "\n",
    "p_d.process_data(isHuman=False, in_file='data/model/human_trials/', out_dir='data/model/no_implicit_cost/')\n",
    "\n",
    "p_d.run_kmeans(direc='data/human/', isHuman=True, k_range=[5])\n",
    "\n",
    "p_d.run_kmeans(in_file='data/model/no_implicit_cost/trials_model.pkl', k_range=[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot figures from main text\n",
    "\n",
    "save = False\n",
    "in_dir = 'data/model/no_implicit_cost/'\n",
    "out_dir = 'figs/no_implicit_cost/'\n",
    "out_suffix = ''\n",
    "in_dir_human = 'data/human/1.0/'\n",
    "in_suffix = ''\n",
    "remove_participants = False\n",
    "\n",
    "centroid_order_mod = [3,2,1,0]\n",
    "centroid_order = [3,4,1,0,2]\n",
    "f=mf.centroids(      save=save, in_dir=in_dir, out_dir=out_dir, out_suffix=out_suffix, in_dir_human=in_dir_human, in_suffix=in_suffix, centroid_order=centroid_order, centroid_order_mod=centroid_order_mod)\n",
    "mf.strategies_fillbetween(save=save, in_dir=in_dir, out_dir=out_dir, out_suffix=out_suffix, in_dir_human=in_dir_human, in_suffix=in_suffix)\n",
    "mf.heatmaps(       save=save, in_dir=in_dir, out_dir=out_dir, out_suffix=out_suffix, in_dir_human=in_dir_human, in_suffix=in_suffix)\n",
    "\n",
    "params = ['nr_clicks','processing_pattern','click_var_outcome','click_var_gamble','payoff_relative']\n",
    "labels = ['Information Gathered','Alternative vs. Attribute','Attribute Variance','Alternative Variance','Relative Performance']\n",
    "ylims = [(2,20),(-1,-.3),(0,.2),(0,.06),(0,1.03)]\n",
    "idxs = [[[0,1],[2,3],[4]]\n",
    "for idx in idxs:\n",
    "    mf.condition_plots_n(save=save, in_dir=in_dir, out_dir=out_dir, out_suffix=out_suffix, in_dir_human=in_dir_human, in_suffix=in_suffix, params=[params[i] for i in idx], labels=[labels[i] for i in idx], ylims=[ylims[i] for i in idx], big=True, remove_participants=remove_participants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrices and LDA projections\n",
    "\n",
    "df_trials = pd.read_pickle('data/human/1.0/trials.pkl')\n",
    "df_trials_mod = pd.read_pickle('data/model/no_implicit_cost/trials_model.pkl')\n",
    "\n",
    "\n",
    "# confusion matrix of strategy vs cluster labels\n",
    "mf.plt_conf_mat_clusterVsKmeans(isHuman=True, save=False)\n",
    "mf.plt_conf_mat_clusterVsKmeans(isHuman=False, save=False, df=df_trials_mod)\n",
    "\n",
    "\n",
    "# LDA to illustrate clusters\n",
    "mf.lda(isHuman=False, pca=False, save=False, df=df_trials_mod)\n",
    "mf.lda(isHuman=True, pca=False, save=False, df=df_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze sources of under-performance, with or without fit cost, with or without excluding participants, and plot confusion matrices\n",
    "\n",
    "df_m = pd.read_csv('data/model/human_trials/trials_model.csv', usecols=['problem_id','cost','strategy','net_payoff','payoff_gross','payoff_perfect'])\n",
    "df_m2 = pd.read_csv('data/model/human_trials_fitcost/trials_model.csv', usecols=['problem_id','cost','strategy','net_payoff','payoff_gross','payoff_perfect'])\n",
    "df_m3 = pd.read_csv('data/model/human_trials_fitcost_exclude/trials_model.csv', usecols=['problem_id','cost','strategy','net_payoff','payoff_gross','payoff_perfect'])\n",
    "\n",
    "confusion_mat1, points_lost1, relative_perf_lost1, count_tot1, kappa_lists1 = p_d.calc_confusion_mat(df, df_m)\n",
    "confusion_mat2, points_lost2, relative_perf_lost2, count_tot2, kappa_lists2 = p_d.calc_confusion_mat(df, df_m2)\n",
    "confusion_mat3, points_lost3, relative_perf_lost3, count_tot3, kappa_lists3 = p_d.calc_confusion_mat(df, df_m3)\n",
    "\n",
    "from statsmodels.stats.inter_rater import cohens_kappa, fleiss_kappa\n",
    "print('no_implicit_cost, all:\\n',cohens_kappa(confusion_mat1))\n",
    "print('w/ implicit cost, all:\\n',cohens_kappa(confusion_mat2))\n",
    "print('no_implicit_cost, top4:\\n',cohens_kappa(confusion_mat1[:4,:4]))\n",
    "print('w/ implicit cost, top4:\\n',cohens_kappa(confusion_mat2[:4,:4]))\n",
    "\n",
    "for relative_perf_lost in [relative_perf_lost1, relative_perf_lost2, relative_perf_lost3]:\n",
    "    lost = relative_perf_lost/sum(sum(relative_perf_lost))\n",
    "    print('imperfect strategy execution: ',np.sum(np.eye(6)*lost))\n",
    "    print('imperfect strategy selection: ',np.sum((-np.eye(6)+1)*lost))\n",
    "\n",
    "\n",
    "for confusion_mat, points_lost, relative_perf_lost, count_tot, save_suffix, savedir, save_suffix in [\n",
    "    [confusion_mat1, points_lost1, relative_perf_lost1, count_tot1, 'figs/human_trials/', ''], \n",
    "    [confusion_mat2, points_lost2, relative_perf_lost2, count_tot2, 'figs/human_trials_fitcost/', '_fitcost'], \n",
    "    [confusion_mat3, points_lost3, relative_perf_lost3, count_tot3, 'figs/human_trials_fitcost_exclude/', '_fitcost_exclude']]:\n",
    "\n",
    "    save = False\n",
    "\n",
    "    lost = np.round(relative_perf_lost/sum(sum(relative_perf_lost))*100,1)\n",
    "    lost[lost==0] = 0\n",
    "    savename = savedir + 'confusion_mat_pctRelPerf' + save_suffix\n",
    "    mf.plt_conf_mat(confusion_mat, points_lost=lost, save=save, savename=savename)\n",
    "\n",
    "    q = confusion_mat/confusion_mat.sum(axis=1)[:,None]; q[np.isnan(q)]=0\n",
    "    lost = np.divide(-points_lost,count_tot); lost[np.isnan(lost)]=0\n",
    "    savename = savedir + 'confusion_mat_avgPts4' + save_suffix\n",
    "    mf.plt_conf_mat(q[:4,:4], points_lost=lost[:4,:4], save=save, savename=savename)\n",
    "\n",
    "    pct_points_lost = np.round(points_lost/sum(sum(points_lost))*100,1)\n",
    "    pct_points_lost[pct_points_lost==0] = 0\n",
    "    savename = savedir + 'confusion_mat_pctPts' + save_suffix\n",
    "    mf.plt_conf_mat(confusion_mat, points_lost=pct_points_lost, save=save, savename=savename)\n",
    "\n",
    "    savename = savedir + 'confusion_mat_totalPts' + save_suffix\n",
    "    mf.plt_conf_mat(confusion_mat, points_lost=points_lost, save=save, savename=savename)\n",
    "\n",
    "    savename = savedir + 'confusion_mat_avgPts' + save_suffix\n",
    "    mf.plt_conf_mat(q, points_lost=lost, save=save, savename=savename)\n",
    "\n",
    "    savename = savedir + 'confusion_mat_pctPtsNorm' + save_suffix\n",
    "    mf.plt_conf_mat(q, points_lost=pct_points_lost, save=save, savename=savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relative performance, controlling for different sources of underperformance\n",
    "\n",
    "save = False\n",
    "in_dir = 'data/model/human_trials_fitcost_exclude/'\n",
    "out_dir = 'figs/human_trials_fitcost_exclude/'\n",
    "out_suffix = ''\n",
    "in_dir_human = 'data/human/1.0/'\n",
    "in_suffix = ''\n",
    "\n",
    "params = ['payoff_relative']\n",
    "labels = ['Relative Performance']\n",
    "ylims = [(0,1.03)]\n",
    "idx = [0]\n",
    "for tf in [True]:\n",
    "    remove_participants = tf\n",
    "    perf = mf.condition_plots_n(save=save, in_dir=in_dir, out_dir=out_dir, out_suffix=out_suffix, in_dir_human=in_dir_human, in_suffix=in_suffix, params=[params[i] for i in idx], labels=[labels[i] for i in idx], ylims=[ylims[i] for i in idx], big=True, remove_participants=remove_participants)\n",
    "\n",
    "    pct_optimal = perf[3] / perf[0]\n",
    "    rel_perf_diff = perf[0] - perf[3]\n",
    "    subopt_info_gathering = perf[0] - perf[1]\n",
    "    subopt_info_gathering_pct = subopt_info_gathering / rel_perf_diff *100\n",
    "    subopt_strat_exec = perf[2] - perf[3]\n",
    "    subopt_strat_exec_pct = subopt_strat_exec / rel_perf_diff *100\n",
    "    subopt_strat_selec = perf[1] - perf[2]\n",
    "    subopt_strat_selec_pct = subopt_strat_selec / rel_perf_diff *100\n",
    "\n",
    "    print(pct_optimal)\n",
    "    print(subopt_info_gathering, subopt_info_gathering_pct)\n",
    "    print(subopt_strat_exec, subopt_strat_exec_pct)\n",
    "    print(subopt_strat_selec, subopt_strat_selec_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mixed-effects linear regression of behavioral features on environmental parameters\n",
    "\n",
    "df = pd.read_pickle('data/human/1.0/trials.pkl')\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "df['payoff_relative'] = df['payoff_gross'] / df['payoff_perfect']\n",
    "for p in ['nr_clicks','payoff_relative','processing_pattern','click_var_gamble','click_var_outcome']:\n",
    "    print('=========================')\n",
    "    for c in ['sigma','alpha','cost']:\n",
    "        print('*********************')\n",
    "        print(p,c)\n",
    "\n",
    "        # normalize for standardized regression coefficients\n",
    "        df_z = df.dropna(subset=[p]); df_z[p] = (df_z[p] - df_z[p].mean())/df_z[p].std(ddof=0)\n",
    "        \n",
    "        res = smf.mixedlm(p+'~'+c, df_z, groups=df_z[\"pid\"]).fit()\n",
    "        \n",
    "        print('betas: ',res.params)\n",
    "        print('p-values: ',res.pvalues)\n",
    "        print(res.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute chi^2 for strategy frequencies\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from chisq_and_posthoc_corrected import chisq_and_posthoc_corrected\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from statsmodels.stats import multitest\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "strategies = ['SAT_TTB','TTB_SAT','Other','TTB','Rand','WADD']\n",
    "\n",
    "sigmas = np.sort(df.sigma.unique())\n",
    "alphas = np.sort(df.alpha.unique())\n",
    "costs = np.sort(df.cost.unique())\n",
    "\n",
    "print('SIGMA')\n",
    "for g in strategies:\n",
    "    print(g)\n",
    "    chi_table = [[sum(df[df.sigma==sigmas[0]][g].values==True), sum(df[df.sigma==sigmas[0]][g].values==False)], [sum(df[df.sigma==sigmas[1]][g].values==True), sum(df[df.sigma==sigmas[1]][g].values==False)]]\n",
    "    print(chi2_contingency(chi_table))\n",
    "    print('effect size: ', proportion_effectsize(np.mean(df[df.sigma==sigmas[0]][g]), np.mean(df[df.sigma==sigmas[1]][g])))\n",
    "        \n",
    "print('ALPHA')\n",
    "for g in strategies:\n",
    "    print(g)\n",
    "    p_vals = []\n",
    "    chi2 = []\n",
    "    chi_tables = []\n",
    "    for i in list(combinations([0,1,2,3,4],2)):\n",
    "        chi_table = [[sum(df[df.alpha==alphas[i[0]]][g].values==True), sum(df[df.alpha==alphas[i[0]]][g].values==False)], [sum(df[df.alpha==alphas[i[1]]][g].values==True), sum(df[df.alpha==alphas[i[1]]][g].values==False)]]\n",
    "        ch, p, _, _ = chi2_contingency(chi_table)\n",
    "        p_vals.append(p)\n",
    "        chi2.append(ch)\n",
    "        chi_tables.append(chi_table[0]); chi_tables.append(chi_table[1])\n",
    "    reject_list, corrected_p_vals, _, _ = multipletests(p_vals, method='fdr_bh', alpha=0.05)\n",
    "    ch, p, dof, _ = chi2_contingency(chi_tables)\n",
    "    print('chi^2:',ch,'p-value:',p,'Deg. of Freedom:',dof)\n",
    "    print(pd.DataFrame({'pairs':list(combinations([0,1,2,3,4],2)),'chi2':chi2,'p_val':p_vals,'p_val_corr':corrected_p_vals,'reject':reject_list}))\n",
    "    \n",
    "    for i in range(4):\n",
    "        print('effect size for ',str(i),' and ',str(i+1),': ', proportion_effectsize(np.mean(df[df.alpha==alphas[i]][g]), np.mean(df[df.alpha==alphas[i+1]][g])))\n",
    "    \n",
    "\n",
    "print('COST')\n",
    "for g in strategies:\n",
    "    print(g)\n",
    "    p_vals = []\n",
    "    chi2 = []\n",
    "    chi_tables = []\n",
    "    for i in list(combinations([1,2,3,4],2)):\n",
    "        chi_table = [[sum(df[df.cost==costs[i[0]]][g].values==True), sum(df[df.cost==costs[i[0]]][g].values==False)], [sum(df[df.cost==costs[i[1]]][g].values==True), sum(df[df.cost==costs[i[1]]][g].values==False)]]\n",
    "        ch, p, _, _ = chi2_contingency(chi_table)\n",
    "        p_vals.append(p)\n",
    "        chi2.append(ch)\n",
    "        chi_tables.append(chi_table[0]); chi_tables.append(chi_table[1])\n",
    "    reject_list, corrected_p_vals, _, _ = multipletests(p_vals, method='fdr_bh', alpha=0.05)\n",
    "    ch, p, dof, _ = chi2_contingency(chi_tables)\n",
    "    print('chi^2:',ch,'p-value:',p,'Deg. of Freedom:',dof)\n",
    "    print(pd.DataFrame({'pairs':list(combinations([1,2,3,4],2)),'chi2':chi2,'p_val':p_vals,'p_val_corr':corrected_p_vals,'reject':reject_list}))\n",
    "\n",
    "    for i in range(4):\n",
    "        print('effect size for ',str(i),' and ',str(i+1),': ', proportion_effectsize(np.mean(df[df.cost==costs[i]][g]), np.mean(df[df.cost==costs[i+1]][g])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# participant demographics\n",
    "\n",
    "# load participant demographic file\n",
    "participants = pd.read_csv('data/processed/1.0/participants.csv')\n",
    "\n",
    "print('Participants: ',len(participants),\n",
    "     '\\nFemales: ',sum(participants.gender=='female'),\n",
    "     '\\nAge ',np.mean(participants.age),' +/- ',np.std(participants.age),' ',min(participants.age),'-',max(participants.age),\n",
    "     '\\nBonus: ',np.mean(participants.bonus),' +/- ',np.std(participants.bonus),' ',min(participants.bonus),'-',max(participants.bonus),\n",
    "     '\\nExperiment length (minutes): ',np.mean(participants.total_time)/60000,' +/- ',np.std(participants.total_time)/60000,' ',min(participants.total_time)/60000,'-',max(participants.total_time)/60000)\n",
    "\n",
    "# choice performance (given clicking behavior)\n",
    "df = trials\n",
    "chose_highest_EV = sum(np.abs(np.diff(df[['EV_chosen','EV_max']]))<0.01)/len(df)\n",
    "print('Participants chose highest EV gamble ',chose_highest_EV,'% of trials')\n",
    "points_lost_per_trial = np.mean(np.diff(df[['EV_chosen','EV_max']]))\n",
    "print('Participants lost ',points_lost_per_trial,' points per trial from not selecting the highest EV gamble')\n",
    "percent_lost_per_trial = 1 - sum(df['EV_chosen'])/sum(df['EV_max'])\n",
    "print('That''s ',percent_lost_per_trial,'% below the highest EV gamble')\n",
    "print('Participants are ',mean_by_condition.net_payoff.mean()/mean_by_condition_mod.net_payoff.mean(),' % optimal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
